{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib, os, zipfile, glob\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open('HillaryClinton.zip', 'wb').write(urllib.urlopen('https://github.com/ilyakats/CUNY-DATA620/blob/master/data/HillaryClinton.zip?raw=true').read())\n",
    "open('DonaldTrump.zip', 'wb').write(urllib.urlopen('https://github.com/ilyakats/CUNY-DATA620/blob/master/data/DonaldTrump.zip?raw=true').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile(os.path.realpath('HillaryClinton.zip'), 'r')\n",
    "zip_ref.extractall(os.path.realpath(''))\n",
    "zip_ref = zipfile.ZipFile(os.path.realpath('DonaldTrump.zip'), 'r')\n",
    "zip_ref.extractall(os.path.realpath(''))\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.unlink('DonaldTrump.zip')\n",
    "os.unlink('HillaryClinton.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_files = glob.glob(os.path.realpath('DonaldTrump') + \"\\\\*.txt\")\n",
    "\n",
    "with open(\"DonaldTrump.txt\", \"wb\") as DonaldTrump:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            DonaldTrump.write(infile.read())\n",
    "\n",
    "read_files = glob.glob(os.path.realpath('HillaryClinton') + \"\\\\*.txt\")\n",
    "\n",
    "with open(\"HillaryClinton.txt\", \"wb\") as HillaryClinton:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            HillaryClinton.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('DonaldTrump.txt', 'rU')\n",
    "DT_text = f.read()\n",
    "\n",
    "f = open('HillaryClinton.txt', 'rU')\n",
    "HC_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 2. How many total unique words are in the corpus? (Please feel free to define unique words in any interesting, defensible way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "tokens = RegexpTokenizer(r'\\w+').tokenize(HC_text)\n",
    "lower_words = [w.lower() for w in tokens]\n",
    "filtered_words = [word for word in lower_words if word not in stopwords.words('english')]\n",
    "stemmed_words = [PorterStemmer().stem(word) for word in filtered_words]\n",
    "HC_results = Counter(stemmed_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('%d unique stemmed words' % len(HC_results))\n",
    "pd.DataFrame(zip(HC_results.values(),HC_results.keys())).sort_values(by = [0], axis=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = RegexpTokenizer(r'\\w+').tokenize(DT_text)\n",
    "lower_words = [w.lower() for w in tokens]\n",
    "filtered_words = [word for word in lower_words if word not in stopwords.words('english')]\n",
    "stemmed_words = [PorterStemmer().stem(word) for word in filtered_words]\n",
    "DT_results = Counter(stemmed_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('%d unique stemmed words' % len(DT_results))\n",
    "pd.DataFrame(zip(DT_results.values(),DT_results.keys())).sort_values(by = [0], axis=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.\tTaking the most common words, how many unique words represent half of the total words in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(zip(HC_results.keys(), HC_results.values()))\n",
    "x.columns = ['word', 'counts']\n",
    "x = x.sort_values(['counts'], ascending=[0])\n",
    "x['cum_sum'] = x.counts.cumsum()\n",
    "x['cum_perc'] = x.cum_sum/x.counts.sum()\n",
    "HC_word_results = x\n",
    "HC_word_results_top50 = HC_word_results[HC_word_results['cum_perc'] <= .50]\n",
    "\n",
    "print ('%d unique stemmed words represent the top half of words in the corpus' % len(HC_word_results_top50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(zip(DT_results.keys(), DT_results.values()))\n",
    "x.columns = ['word', 'counts']\n",
    "x = x.sort_values(['counts'], ascending=[0])\n",
    "x['cum_sum'] = x.counts.cumsum()\n",
    "x['cum_perc'] = x.cum_sum/x.counts.sum()\n",
    "DT_word_results = x\n",
    "DT_word_results_top50 = DT_word_results[DT_word_results['cum_perc'] <= .50]\n",
    "\n",
    "print ('%d unique stemmed words represent the top half of words in the corpus' % len(DT_word_results_top50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 4. Identify the 200 highest frequency words in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HC_word_results_top200 = HC_word_results.head(200)\n",
    "DT_word_results_top200 = DT_word_results.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 5. Create a graph that shows the relative frequency of these 200 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "HC_word_results_top200.plot( x = 'word', y = 'cum_sum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_word_results_top200.plot( x = 'word', y = 'cum_sum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = RegexpTokenizer(r'\\w+').tokenize(HC_text)\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance(\"Trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = RegexpTokenizer(r'\\w+').tokenize(DT_text)\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance(\"Clinton\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
